{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fee8807",
   "metadata": {},
   "source": [
    "# 자연어 처리 개요\n",
    "\n",
    "- 텍스트 분류\n",
    "- 텍스트 유사도\n",
    "- 텍스트 생성\n",
    "- 기계 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594bfb6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 단어표현\n",
    "\n",
    "자연어 처리는 컴퓨터가 인간의 언어를 이해하고 분석 가능한 모든 분야를 말한다.\n",
    "따라서 자연어 처리의 가장 기본적인 문제는 '어떻게 자연어를 컴퓨터에게 인식시킬 수 있을까?'다.\n",
    "우선 컴퓨터가 텍스트를 인식하는 기본적인 방법을 알아보자.\n",
    "컴퓨터는 텍스트뿐만 아니라 모든 값을 읽을 때 이진화된 값으로 받아들인다.\n",
    "즉, 0, 1로만 구성된 값으로 인식할 수 있는데, 텍스트는 그중 \"유니코드\"라는 방법 혹은 영어의 경우 \"아스키 코드\"라는 방식을 통해 인식할 수 있다.\n",
    "먼저 유니코드의 경우 어떻게 인식하는지 알아보자.\n",
    "\"언어\"라는 텍스트를 유니코드 방식으로 컴퓨터가 인식한다면 다음과 같은 형태로 입력된다.\n",
    "\n",
    "- '언' : 1100010110111000\n",
    "- '어' : 1100010110110100\n",
    "\n",
    "두 글자를 위와 같이 이진화된 값으로 인식하는데, 자연어 처리에 이러한 방식을 그대로 사용하기에는 문제가 있다.\n",
    "문자를 이진화한 값의 경우 언어적인 특성이 전혀 없이 컴퓨터가 문자를 인식하기 위해 만들어진 값이므로 자연어 처리를 위해 만드는 모델에 적용하기에는 부적합하다.\n",
    "그렇다면 어떤 방식으로 텍스트를 표현해야 자연어 처리 모델에 적용할 수 있을까?\n",
    "\n",
    "이러한 질문의 답을 찾는 것이 \"단어 표현\" 분야다.\n",
    "텍스트를 자연어 처리를 위한 모델에 적용할 수 있게 언어적인 특성을 반영해서 단어를 수치화하는 방법을 찾는 것이다.\n",
    "그리고 이렇게 단어를 수치화할 때는 단어를 주로 벡터로 표현한다.\n",
    "따라서 단어 표현은 \"단어 임베딩\" 또는 \"단어 벡터\"로 표현하기도 한다.\n",
    "단어 표현에는 다양한 방법이 있고 계속해서 연구되는 분야이기 때문에 하나의 정답이 있는 것은 아니지만 이 책에서는 많이 사용하는 방법 위주로 하나씩 알아가보자.\n",
    "\n",
    "단어를 표현하는 가장 기본적인 방법은 원-핫 인코딩 방식이다.\n",
    "단어를 하나의 벡터로 표현하는 방법인데, 각 값은 0 혹은 1만 갖는다.\n",
    "즉, 각 단어는 0과 1값만 가지는 벡터로 표현되는데, 이름에서 알 수 있는 벡터 값 가운데 하나만 1이라는 값을 가지고 나머지는 모두 0 값을 가지는 방식이다.\n",
    "여기서 1이 되는 것은 갓 단어가 어떤 단어인지 알려주는 인덱스가 된다.\n",
    "\n",
    "예를 들어, 6개의 단어(남자, 여자, 엄마, 아빠, 삼촌, 이모)를 알려줘야 한다고 했을 때 원-핫 인코딩 방식으로 각 단어를 표현한다고 해보자.\n",
    "이때 각 단어를 표현하는 벡터의 크기는 6이 된다.\n",
    "따라서 각 단어의 벡터는 총 6개의 값을 가지는데, 이 중에서 하나만 1이 된다.\n",
    "이때 1이 되는 값을 통해 각 단어가 어떤 단어인지 알 수 있다.\n",
    "아래의 그림과 같이 남자는 \\[1, 0, 0, 0, 0, 0\\]으로, 여자는 \\[0, 1, 0, 0, 0, 0\\]으로, 엄마는 \\[0, 0, 1, 0, 0, 0\\]으로, 아빠는 \\[0, 0, 0, 1, 0, 0\\]으로, 삼촌은 \\[0, 0, 0, 0, 1, 0\\]으로, 이모는 \\[0, 0, 0, 0, 0, 1\\]로 표현된다.\n",
    "\n",
    "즉, 원-핫 인코딩 방식은 각 단어의 인덱스를 정한 후 각 단어의 벡터에서 그 단어에 해당하는 인덱스의 값을 1로 표현하는 방식이다.\n",
    "방법 자체가 매우 간단하고 이해하기도 쉽다는 장점이 있다.\n",
    "하지만 이 방식에는 결정적인 두 가지 문제점이 있다.\n",
    "위의 예시에서는 총 6개의 단어만 표현하면 되지만 실제로 자연어 처리 문제를 해결할 때는 수십만, 수백만 개가 넘는 단어를 표현해야 한다.\n",
    "이 경우에는 각 단어 벡터의 크기가 너무 커지기 때문에 공간을 많이 사용하고, 큰 공간에 비해 실제 사용하는 값은 1이 되는 값 하나뿐이므로 매우 비효율적이다.\n",
    "또 다른 문제점은 이러한 표현 방식은 단순히 단어가 뭔지만 알려 줄 수 있고, 벡터값 자체에는 단어의 의미나 특성 같은 것들이 전혀 표현되지 않는다는 것이다.\n",
    "\n",
    "따라서 이러한 원-핫 인코딩 방식의 문제점인 단어 벡터의 크기가 너무 크고 값이 희소하다는 문제와 단어 벡터가 단어의 의미나 특성을 전혀 표현할 수 없다는 문제를 해결하기 위해 다른 인코딩 방법들이 제안됐다.\n",
    "즉, 벡터의 크기가 작으면서도 벡터가 단어의 의미를 표현할 수 있는 방법들인데, 이러한 방법들은 **분포 가설**을 기반으로 한다.\n",
    "분포 가설이란 \"같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다\"라는 개념이다.\n",
    "따라서 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단하는 방법인데, 크게 두 가지 방법으로 나뉜다.\n",
    "\n",
    "특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 세는 방법인 **카운터 기반 방법**과 신경망 등을 통해 문맥 안의 단어들을 예측하는 방법인 **예측 방법**으로 나뉜다.\n",
    "이어서 이 두 가지 분류에 대해 어떤 단어 표현 방법이 있는지 알아보고, 어떤 방식으로 단어를 표현하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc6717",
   "metadata": {},
   "source": [
    "### 카운트 기반 방법\n",
    "\n",
    "카운트 기반 방법으로 단어를 표현한다는 것은 어떤 글의 문맥 안에 단어가 동시에 등장하는 횟수를 세는 방법이다.\n",
    "여기서 동시에 등장하는 횟수를 동시 출현 혹은 공기라고 부르고 영어로는 Co-occurrence라고 한다.\n",
    "카운트 기반 방법은 기본적으로 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법을 사용하는 방식이다.\n",
    "\n",
    "- 특이값 분해(Singular Value Decomposition, SVD)\n",
    "- 잠재의미분석(Latent Semantic Analysis, LSA)\n",
    "- Hyperspace Analogue to Language(HAL)\n",
    "- Hellinger PCA(Principal Component Analysis)\n",
    "\n",
    "위의 방법은 모두 동시 출현 행렬(Co-occurrence Matrix)을 만들고 그 행렬들을 변형하는 방식인데,\n",
    "이 책에서는 동시 출현 행렬까지만 만들어 보고 행렬을 통해 다시 단어 벡터로 만드는 방법에 대해서는 다루지 않는다.\n",
    "\n",
    "다음 예시를 가지고 동시 출현 행렬을 만들어 보자\n",
    "\n",
    "- 성진과 창욱은 야구장에 갔다.\n",
    "- 성진과 태균은 도서관에 갔다.\n",
    "- 성진과 창욱은 공부를 좋아한다.\n",
    "\n",
    "위의 문장들을 가지고 동시 출현 행렬을 만들려면 같은 문장 안에 단어가 함께 출현한 횟수를 센 후 다음과 같은 행렬을 만들면 된다.\n",
    "\n",
    "|-|성진과|창욱은|태균은|야구장에|도서관에|공부를|갔다.|좋아한다.|\n",
    "|-|-|-|-|-|-|-|-|-|\n",
    "|성진과|0|2|1|0|0|0|0|0|\n",
    "|창욱은|2|0|0|1|0|1|0|0|\n",
    "|태균은|1|0|0|0|1|0|0|0|\n",
    "|야구장에|0|1|0|0|0|0|1|0|\n",
    "|도서관에|0|0|1|0|0|0|1|0|\n",
    "|공부를|0|1|0|0|0|0|0|1|\n",
    "|갔다.|0|0|0|1|1|0|0|0|\n",
    "|좋아한다.|0|0|0|0|0|1|0|0|\n",
    "\n",
    "이렇게 만들어진 동시 출현 행렬을 토대로 특이값 분해 방법 등을 사용해 단어 벡터를 만들면 된다.\n",
    "이러한 카운트 기반 방법은 장점은 우선 빠르다는 점이다.\n",
    "여기서 말하는 '빠르다'는 것은 우리가 만들어야 할 단어 벡터가 많아질수록 사용하는 방법에 따라 시간이 많이 소용되는데,\n",
    "이러한 방식의 장점은 적은 시간으로 단어 벡터를 만들 수 있다는 것이다.\n",
    "그리고 이러한 방식은 예측 방법에 비해 좀 더 이전에 만들어진 방법이지만 데이터가 많을 경우에는 단어가 잘 표현되고 효율적이어서 아직까지도 많이 사용하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fb460",
   "metadata": {},
   "source": [
    "### 예측 방법\n",
    "\n",
    "예측 기반 방법이란 신경만 구조 혹은 어떠한 모델을 사용해 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식이다.\n",
    "예측 방법에는 다음과 같은 것이 있다.\n",
    "- Word2vec\n",
    "- NNLM(Neural Network Language Model)\n",
    "- RNNLM(Recurrent Neural Network Language Model)\n",
    "\n",
    "여러 예측 기반 방법 중에서 단어 표현 방법으로 가장 많이 사용 되는 `Word2vec`에 대해 자세히 알아보자.\n",
    "`Word2vec`은 CBOW 와 Skip-Gram이라는 두 가지 모델로 나뉜다.\n",
    "두 모델은 각각 서로 반대되는 개념으로 생각하면 되는데, CBOW의 경우 어떤 단어를 문백 안의 주변 단어들을 통해 예측하는 방법이다.\n",
    "반대로 Skip-Gram의 경우에는 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 방법이다.\n",
    "\n",
    "예시를 들어 비교해 보자. 다음과 같은 문장이 있다고 하자.\n",
    "\n",
    "- 창욱은 냉장고에서 음식을 꺼내서 먹었다.\n",
    "\n",
    "이때 CBOW는 주변 단어를 통해 하나의 단어를 예측하는 모델이다.\n",
    "즉, 다음 문장의 빈칸을 채우는 모델이라고 생각하면 된다.\n",
    "\n",
    "- 창욱은 냉장고에서 \\_\\_\\_ 꺼내서 먹었다.\n",
    "\n",
    "반대로 Skip-Gram은 하나의 단어를 가지고 주변에 올 단어를 예측하는 모델이다.\n",
    "다음 문장에서 빈칸을 채운다고 생각하면 된다.\n",
    "\n",
    "- \\_\\_\\_ \\_\\_\\_\\_\\_ 음식을 \\_\\_\\_ \\_\\_\\_\\_\n",
    "\n",
    "두 모델은 위와 같이 단어들을 예측하면서 단어 벡터를 계속해서 학습한다.\n",
    "모델의 전체적인 구조는 다음 그림과 같다.\n",
    "\n",
    "<img src=\"cbow-skip-gram.png\" alt=\"cbow-skip-gram\" style=\"width: 500px;\" />\n",
    "\n",
    "각 모델의 학습 방법에 대해 설명하면 CBOW의 경우 다음과 같은 순서로 학습한다.\n",
    "\n",
    "1. 각 주변 단어들을 원-핫 벡터로 만들어 입력값으로 사용한다(Input Layer Vector).\n",
    "2. 가중치 행렬을 각 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-Dimension Hidden Layer).\n",
    "3. 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균-n 차원 벡터를 만든다(Output Layer Vector).\n",
    "4. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다.\n",
    "5. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다.\n",
    "\n",
    "Skip-Gram의 학습 방법도 비슷한 과정으로 진행한다.\n",
    "\n",
    "1. 하나의 단어를 원-핫 벡터로 만들어서 입력값으로 사용한다(Input Layer Vector).\n",
    "2. 가중치 행렬을 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-Dimension Hidden Layer).\n",
    "3. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다(Output Layer Vector).).\n",
    "4. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다.\n",
    "\n",
    "두 모델의 학습 과정이 비슷해 보이지만 확실한 차이점이 있다.\n",
    "CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교한다.\n",
    "Skip-Gram에서는 입력값이 하나의 단어를 사용하고, 학습을 위해 주변의 여러 단어와 비교한다.\n",
    "\n",
    "위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용한다.\n",
    "이처럼 Word2vec의 두 모델은 여러 가지 장점이 있다.\n",
    "기존의 카운트 기반 방법으로 만든 단어 벡터보다 단어 간의 유사도를 잘 측정한다.\n",
    "또 한 가지 장점은 단어들의 복잡한 특징까지도 잘 잡아낸다는 점이다.\n",
    "마지막으로 이렇게 만들어진 단어 벡터는 서로에게 유의미한 관계를 측정할 수 있다는 점인데,\n",
    "예를 들어 4개의 단어(엄마, 아빠, 여자, 남자)를 word2vec 방식을 사용해 단어 벡터로 만들었다고 하자.\n",
    "이때 다음 그림과 같이 '엄마'와 '아빠'라는 다언의 벡터 사이의 거리와 '여자'와 '남자'라는 단어의 벡터 사이의 거리가 같게 나온다.\n",
    "\n",
    "Word2vecDML CBOW와 Skip-Gram 모델 중에서는 보통 Skip-Gram이 성능이 좋아 일반적인 경우 Skip-Gram을 사용한다.\n",
    "하지만 절대적으로 항상 좋은 것은 아니니 두 가지 모두 고려할 만하다.\n",
    "\n",
    "\n",
    "이처럼 카운트 기반 방법과 예측 기반 방법을 통해 단어 벡터를 표현하는데, 보통의 경우 예측 기반 방법의 성능이 좋아서 주로 예측 기반 방법을 사용한다.\n",
    "그리고 두 가지 방법을 모두 포함하는 \"Glove\" 라는 단어 표현 방법 또한 자주 사용한다.\n",
    "\n",
    "단어 표현은 모든 자연어 처리 문제를 해결하는 데 기반이 되는 가장 근본적인 내용이므로 정확하게 이해해야 한다.\n",
    "그리고 항상 가장 좋은 성능을 내는 유일한 방법이 있는 것이 아니라서 각 방법 간에 어떤 차이점이 있는지 항상 염두해 두고 상황에 맞게 사용하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d797ba",
   "metadata": {},
   "source": [
    "### 텍스트 분류\n",
    "\n",
    "### 스팸 분류\n",
    "\n",
    "누든 메일함에 들어갈 때마다 넘치는 스팸메일 때문에 짜증났던 기억이 분명히 있을 것이다.\n",
    "계속해서 들어오는 스팸 메일 때문에 정작 필요한 메일은 확인하기 어려워진다.\n",
    "따라서 자동으로 스팸 메일을 걸럿 사용자에게 일반 메일만 확인할 수 있게 보여주는 것이 중요할 것이다.\n",
    "이처럼 일반 메일과 스팸 메일을 분류하는 문제가 스팸 분류 문제다.\n",
    "\n",
    "여기서 분류해야 할 범주는 스팸 메일과 일반 메일로 2가지다.\n",
    "이러한 스팸 분류 기술들은 이미 많은 이메일 업체에서 제공하고 있다.\n",
    "\n",
    "### 감정 분류\n",
    "\n",
    "감정 분류란 주어진 글에 대해 이 글이 긍정적인지 부정적인지 판단하는 문제다.\n",
    "이 경우 범주는 긍정 혹은 부정이 된다.\n",
    "경우에 따라 범주는 긍정 혹은 부정으로 나뉘는 것이 아니라 중립의 범주가 추가될 수도 있고, 긍정 혹은 부정의 경우에도 정도에 따라 범주를 세분화할 수 있다.\n",
    "문제에서 분류하려는 의도에 따라 범주가 정해지는 것이다.\n",
    "가장 대표적인 감정 분류 문제는 영화 리뷰에 대해 각 리뷰가 긍정적인지 부정적인지 판단하는 문제인데, 다음 장에서 자세히 설명하고 있으니 직접 분류를 해볼 수 있다.\n",
    "\n",
    "### 뉴스 기사 분류\n",
    "인터넷에는 수많은 뉴스가 존재한다.\n",
    "뉵스가 많다는 것은 장점이지만 사용자 입장에서느 어떤 뉴스를 선택해서 봐야 할지도 어려울 것이다.\n",
    "따라서 뉴스 업체는 사용자가 원하는 뉴스를 선택해서 볼 수 있게 범주를 잘 구분지어 분류해 둬야 할 것이다.\n",
    "스포츠, 경제, 사회, 연예 등 다양한 주제의 기사를 각각 주체에 맞게 분류해서 준비해둬야 한다.\n",
    "자연어 처리 기술을 사용한다면 뉴스 기사를 더욱 효율적으로 분류할 수 있다.\n",
    "\n",
    "위의 예시뿐 아니라 텍스트 분류에는 다양한 문제가 있다.\n",
    "분류하는 단위를 글 전체로 하지 않고 각 단어를 한 단위로 분류하는 문제도 있다.\n",
    "예를 들면, 품사 분류(POS tagging) 문제는 각 단어를 기준으로 어떤 품사를 가지는지 분류해야 하는 문제다.\n",
    "\n",
    "그렇다면 텍스트 분류 문제는 어떻게 해결해야 할까?\n",
    "큰 기준으로 본다면 지도 학습을 통해 해결하거나 비지도 학습을 통해 해결할 수 있을 것이다.\n",
    "\n",
    "### 지도 학습을 통한 텍스트 분류\n",
    "\n",
    "지도 학습을 통해 문장 분류를 하는 전체적인 방법은 다음 그림과 같다.\n",
    "\n",
    "<img src=\"https://inforience.net/wp-content/uploads/2021/02/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA-2021-02-09-%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB-9.53.52.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "위 그림과 같이 지도 학습은 글에 대해 각각 속한 범주에 대한 값이 이미 주어져 있다.\n",
    "따라서 주어진 범주로 글들을 모두 학습한 후 학습한 결과를 이용해 새로운 글의 범주를 예측하는 방법이다.\n",
    "\n",
    "예를 들어, 스팸 분류 문제를 지도 학습으로 해결한다고 해보자.\n",
    "이 경우 10,000개의 메일 데이터가 있고, 이 메일 중 스팸 메일은 5,000개이고, 일반 메일이 5,000개디.\n",
    "각 메일에는 해당 메일이 어떤 메일인지 라벨링돼 있다.\n",
    "이제 각 메일에서 특징을 뽑아내서 예측한 뒤 라벨과 맞는지 확인하면서 학습한다.\n",
    "10,000개의 메일을 모두 사용해서 학습한 뒤 이제 새로운 메일에 대해 학습한 모델을 통해 스팸 메일인지 아닌지 예측한다.\n",
    "\n",
    "지도 학습을 통한 문장 분류 모델에는 다양한 종류가 있다.\n",
    "대표적인 지도 학습의 예를 다음과 같다.\n",
    "\n",
    "- 나이브 베이즈 분류\n",
    "- 서포트 벡터 머신\n",
    "- 신경망\n",
    "- 선형 분류\n",
    "- 로지스틱 분류\n",
    "- 랜덤 포레스트\n",
    "\n",
    "이 밖에도 수많은 모델이 있다. 해당 문제에 적합한 모델은 정해져 있는 것이 아니므로 항상 다양한 모델을 사용해 보는 습관을 길러야 한다.\n",
    "\n",
    "### 비지도 학습을 통한 텍스트 분류\n",
    "\n",
    "그러핟면 비지도 학습을 통한 문장 분류는 무엇일까?\n",
    "지도 학습에서는 각 데이터가 어떤 범주의 데이터인지에 대한 값을 이미 가지고 있었다.\n",
    "그래서 그 값을 토대로 모델을 학습시켰지만 비지도 학습에서는 데이터만 존재하고, 각 데이터는 범주가 미리 나눠져 있지 않다.\n",
    "따라서 특징을 찾아내서 적당한 범주를 만들어 각 데이터를 나누면 된다.\n",
    "\n",
    "대표적으로 비지도 학습의 예인 k-평균 군집화를 예로 들어 보자.\n",
    "각 문장 데이터는 다음과 같이 벡터화한 뒤 좌표축에 표현한다.\n",
    "\n",
    "이후 k-평균 군집화 모델을 사용해 데이터를 몇 개의 군집으로 나눈다.\n",
    "여기서는 k값을 4로 지정한다.\n",
    "군집화를 마친 후 각 데이터는 다음 그림과 같이 4개의 군집으로 나뉜다.\n",
    "\n",
    "비지도 학습을 통한 분류는 어떤 특정한 분류가 있는 것이 아니라 데이터의 특성에 따라 비슷한 데이터끼리 묶어주는 개념이다.\n",
    "위의 그림에서도 비슷하게 분포된 데이터끼리 총 4개의 묶음으로 나눠진 모습을 볼 수 있다.\n",
    "이렇게 묶어준다면 각 묶음이 하나의 버무가 되는 것이다.\n",
    "\n",
    "비지도 학습을 통한 텍스트 분류는 텍스트 군집화라고도 불린다.\n",
    "비지도 학습 모델은 다음과 같다.\n",
    "\n",
    "- k-평균 군집화\n",
    "- 계층적 군집화\n",
    "\n",
    "지도 학습과 비지도 학습 중 어떤 방법을 사용할지 결정하는 데 기여하는 가장 큰 기준은 데이터에 정답 라벨이 있느냐 없느냐다.\n",
    "정답 라벨이 있는 경우 지도 학습 방법으로 문제를 해결하면 되고, 정답 라벨이 없다면 비지도 학습을 사용해서 문제를 해결하면 된다.\n",
    "그리고 일반적인 분류의 경우에는 지도 학습을 사용하지만 정확한 범주가 없고 단순히 군집화만 할 경우에는 비지도 학습을 통해 데이터를 군집화하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05205bd3",
   "metadata": {},
   "source": [
    "## 텍스트 유사도\n",
    "\n",
    "### 자카드 유사도\n",
    "\n",
    "\n",
    "### 코사인 유사도\n",
    "\n",
    "\n",
    "### 유클리디언 유사도\n",
    "\n",
    "\n",
    "### 맨하탄 유사도\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6211c5",
   "metadata": {},
   "source": [
    "## 자연어 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affb70c",
   "metadata": {},
   "source": [
    "## 기계 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f7fad",
   "metadata": {},
   "source": [
    "## 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0bbce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
