{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f1ddab",
   "metadata": {},
   "source": [
    "# 자연어 처리 개발 준비\n",
    "## tf.keras.layers.Dense\n",
    "`tf.keras.layers.Dense`에서 Dense는 신경망 구조의 가장 기본적인 형태를 의미한다.\n",
    "즉, 아래의 수식을 만족하는 기본적인 신경망 형태의 층을 만드는 함수이다.\n",
    "\n",
    "$$y = f(Wx+b)$$\n",
    "\n",
    "위의 수식에서 x와 b는 각각 입력 벡터, 편향 벡터이며 W는 가중치 행렬이 된다.\n",
    "즉, 가중치와 입력 벡터를 곱한 후 편향을 더해준다. 그리고 그 값에 f라는 활성화 함수를 적용하는 구조다.\n",
    "위 수식을 그림으로 보면 아래와 같은 은닉층이 없는 간단한 신경망 형태가 된다.\n",
    "\n",
    "\n",
    "<img src=\"DenseLayer.svg\" alt=\"DenseLayer\"/>\n",
    "\n",
    "위 그림에서 왼쪽 노드들이 입력값인 x가 되고 오른쪽 노드들이 y가 된다.\n",
    "그리고 중간에 있는 선이 가중치를 곱하는 과정을 의미하고, 여기서 곱해지는 가중치들이 앞 수식의 W가 된다.\n",
    "\n",
    "이러한 Dense 층을 구성하는 기본적인 방법은 가중치인 W와 b를 각각 변수로 선언한 후 행렬 곱을 통해 구하는 방법이다.\n",
    "다음과 같이 코드를 작성해서 직접 가중치 변수를 모두 정의해야 한다.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([5, 10], -1, 1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(W, x) + b\n",
    "```\n",
    "\n",
    "위왁 ㅏㅌ이 모든 변수들을 선언하고 하나하나 직접 곱하고, 더해야 한다.\n",
    "하지만 텐서플로의 Dense를 이용하면 한 줄로 위의 코드를 작성할 수 있다.\n",
    "이 경우 내부적으로 변수를 생성하고 연산을 진행한다.\n",
    "아울러 인자 설정에 따라 활성화 함수 설정, 초기화 방법, 정규화 방법 등 다양한 기능을 쉽게 사용할 수 있게 구성돼 있다.\n",
    "\n",
    "케라스의 Dense를 사용하려면 우선 객체를 생성해야 한다.\n",
    "\n",
    "```python\n",
    "dense = tf.keras.layers.Dense(...)\n",
    "```\n",
    "\n",
    "이렇게 생성한 Dense 층을 객체에 입력값을 넣어야 한다.\n",
    "입력값을 넣기 위해서는 객체를 생성할 때 함께 넣거나 생성한 후 따로 적용하는 방법이 있다.\n",
    "\n",
    "```python\n",
    "# 방법 1\n",
    "dense = tf.keras.layers.Dense(...)\n",
    "output = dense(input)\n",
    "# 방법 2\n",
    "output = tf.keras.layers.Dense(...)(input)\n",
    "```\n",
    "\n",
    "Dense 층을 만들 때 여러 인자를 통해 가중치와 편향 초기화 방법, 활성화 함수의 종류 등 여러 가지를 옵션으로 정할 수 있다.\n",
    "객체를 생성할 때 지정할 수 있는 인자는 다음과 같다.\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    units, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    bias_constraint=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- units : 출력 값의 크기, Integer 혹은 Long 형태.\n",
    "- activation : 활성화 함수.\n",
    "- use_bias : 편향을 사용할지 여부. Boolean 값 형태.\n",
    "- kernel_initializer : 가중치 초기화 함수\n",
    "- bias_initializer : 편향 초기화 함수\n",
    "- kernel_regularizer : 가중치 정규화 방법\n",
    "- bias_regularizer : 편향 정규화 방법\n",
    "- activity_regularizer : 출력 값 정규화 방법\n",
    "- kernel_constraint : Optimizer에 의해 업데이트 된 이후에 가중치에 적용되는 부가적인 제약 함수\n",
    "- bias_constraint : Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수\n",
    "\n",
    "입력값에 대해 활성화 함수로 시그모이드 함수를 사용하고, 출력 값으로 10개의 값을 출력하는 완전 연결 계층은 다음과 같이 정의하면 된다.\n",
    "\n",
    "10개의 노드를 가지는 은닉층이 있고 최종 출력 값은 2개의 노드가 있는 신경망 구조를 생각해보자.\n",
    "그렇다면 객체를 두개 생성해서 신경망을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c441b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6e015",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Dropout\n",
    "\n",
    "신경망 모델을 만들 때 생기는 여러 문제점 중 대표적인 문제점은 **과적합(overfitting)** 이다.\n",
    "과적합 문제는 정규화 방법을 사용해서 해결하는데, 그중 가장 대표적인 방법이 `Dropout(드롭아웃)`이다.\n",
    "텐서플로는 드롭아웃을 쉽게 모델에 적용할 수 있게 간단한 모듈을 제공하는데,\n",
    "이 모듈을 이용하면 특정 `keras.layers`의 입력값에 드롭아웃을 적용할 수 있다.\n",
    "사용법은 위의 dense 층을 만드는 방법과 유사하게 Dropout 객체를 생성해서 사용하면 된다.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dropout(...)\n",
    "```\n",
    "\n",
    "드롭아웃을 적용할 입력값을 설정해야 한다.\n",
    "앞서 진행했던 것과 입력값을 설정하는 방법은 동일하다.\n",
    "\n",
    "```python\n",
    "# 방법 1\n",
    "dense = tf.keras.layers.Dropout(...)\n",
    "output = dense(input)\n",
    "# 방법 2\n",
    "output = tf.keras.layers.Dropout(...)(input)\n",
    "```\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "```\n",
    "- rate : 드롭아웃을 적용할 확률을 지정한다. 확률 값이므로 0~1 사이의 값을 받는다. 예를 들어 dropout=0.2로 지정하면 전체 입력값 중에서 20%를 0으로 만든다.\n",
    "- noise_shape : 정수형의 1D-tensor 값을 받는다. 여기서 받은 값은 shape을 뜻하는데, 이 값을 지정함으로써 특정 값만 드롭아웃을 적용할 수 있다. 예를 들면, 입력값이 이미지일 때 noise_shape을 지정하면 특정 채널에만 드롭아웃을 적용할 수 있다.\n",
    "- seed : 드롭아웃의 경우 지정된 확률 값을 바탕으로 무작위로 드롭아웃을 적용하는데, 이때 임의의 선택을 위한 시드 값을 의미한다. seeda 값은 정수형이며, 같은 seed 값을 가지는 드롭아웃의 경우 동일한 드롭아웃 결과를 만든다.\n",
    "\n",
    "드롭아웃을 적용하는 과정을 생각해보자.\n",
    "학습 데이터에 과적합되는 상황을 방지하기 위해 학습 시 특정 확률로 노드들의 값을 0으로 만든다.\n",
    "그리고 이러한 과정은 학습할 때만 적용되고 예측 혹은 테스트할 때는 적용되지 않아야 한다.\n",
    "케라스의 Dropout을 사용할 경우 이러한 부분이 자동으로 적용된다.\n",
    "드롭아웃을 적용하는 방법은 아래와 같이 적용시킬 값을 입력 값으로 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6ba2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420a7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units=10, activation=tf.nn.sigmoid)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a9ea3",
   "metadata": {},
   "source": [
    "텐서플로에서 드롭아웃은 `tf.keras.layers`뿐만 아니라 `tf.nn`에도 있는데,\n",
    "두 모델의 차이점은 `tf.keras.layers.Dropout`의 경우 확률을 0.2로 지정했을 때 노드의 20%를 0으로 만드는 데 비해\n",
    "`tf.nn.Dropout`의 경우 확률을 0.2로 지정했을 때 80% 값을 0으로 만든다는 것이다.\n",
    "\n",
    "함수를 사용하는 방법을 알아보자.\n",
    "이전의 Dense 층 예제인 신경망 구조에서 처음 입력 값에 드롭아웃을 적용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e38435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=INPUT_SIZE)\n",
    "dropput = tf.keras.layers.Dropout(rate=0.2)(inputs)\n",
    "hidden = tf.keras.layers.Dense(units=10, activation=tf.nn.sigmoid)(dropput)\n",
    "output = tf.keras.layers.Dense(units=2, activation=tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8216f6",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Conv1D\n",
    "\n",
    "이번에는 합성곱 연산 중 `Conv1D`에 대해서 알아보자.\n",
    "텐서플로의 합성곱 연산은 `Conv1D`, `Conv2D`, `Conv3D`로 나뉘어지는데 우선 이 세개가 어떤 차이점이 잇는지 알아보자.\n",
    "\n",
    "|-|합성곱의 방향|출력값|\n",
    "|-|-|-|\n",
    "|Conv1D|한 방향(가로)|1-D Array(Vector)|\n",
    "|Conv2D|두 방향(가로, 세로)|2-D Array(matrix)|\n",
    "|Conv3D|세 방향(가로, 세로, 높이)|3-D Array(tensor)|\n",
    "\n",
    "위의 표에서 나온 출력값의 경우 실제 합성곱 출력값과 동일하진 않다.\n",
    "배치 크기와 합성곱이 적용되는 필터의 개수도 고려해야 하기 때문에 출력값이 위와 동일하게 나오지 않는 것이다.\n",
    "위의 경우 단순히 배치의 경우, 고려하지 않고 합성곱 필터를 하나만 적용했을 때라고 생각하면 된다.\n",
    "\n",
    "<img src=\"conv1d.png\" alt=\"conv1d\" style=\"width: 300px;\"/>\n",
    "\n",
    "그림을 보면 빨간색 사격형이 하나의 필터가 된다.\n",
    "이 필터가 가로 방향으로만 옮겨가면서 입력값에 대해 합성곱을 수행한다. 연산 결과들이 모여서 최종 출력 값이 나온다.\n",
    "따라서 출력 값은 하단에 위치한 것과 같은 1차원 벡터가 된다.\n",
    "\n",
    "자연어 처리 분야에서 사용하는 합성곱의 경우 각 단어 벡터의 차원 전체에 대해 필터를 적용시기키 위해 주로 `Conv1D`를 사용한다.\n",
    "\n",
    "```python\n",
    "# 방법 1\n",
    "conv1d = tf.keras.layers.Conv1D(...)\n",
    "output = conv1d(input)\n",
    "\n",
    "# 방법 2\n",
    "output = tf.keras.layers.Conv1D(input)\n",
    "```\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    filters, kernel_size, strides=1, padding='valid',\n",
    "    data_format='channels_last', dilation_rate=1, groups=1,\n",
    "    activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    bias_constraint=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- filters : 필터의 개수로서, 정수형으로 지정한다. 출력의 차원 수를 나타낸다.\n",
    "- kernel_size : 필터의 크기로서, 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 합성곱이 적용되는 윈도의 길이를 나타낸다.\n",
    "- strides : 적용할 스트라이드의 값으로서 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 1이 아닌 값을 지정할 경우 dilation_rate는 1 이외의 값을 지정하지 못한다.\n",
    "- padding : 패딩 방법을 정한다. \"VALID\" 또는 \"SAME\"을 지정할 수 있다.\n",
    "- data_format : 데이터의 표현 방법을 선택한다. \"channel_last\" 혹은 \"channel_first\"를 지정할 수 있다. channel_list의 경우 데이터는(batch, length, channels) 형태여야 하고, channel_first의 경우 데이터는 (batch, channels, length) 형태여야 한다.\n",
    "- dilation_rate : dilation 합성곱 사용 시 적용할 dilation 값으로서 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 1이 아닌 값을 지정하면 strides 값으로 1이외의 값을 지정하지 못한다.\n",
    "- groups : 채널 축을 따라 입력이 분할되는 그룹 수를 지정하는 양의 정수입니다. 각 그룹은 필터/그룹 필터와 별도로 결합됩니다. 출력은 채널 축을 따라 모든 그룹 결과를 연결한 것입니다. 입력 채널과 필터는 모두 그룹으로 나눌 수 있어야 합니다.\n",
    "- activation : 활성화 함수\n",
    "- use_bias : 편향을 사용할지 여부.\n",
    "- kernel_initializer : 가중치 초기화 함수\n",
    "- bias_initializer : 편향 초기화 함수\n",
    "- kernel_regularizer : 가중치 정규화 방법\n",
    "- bias_regularizer : 편향 정규화 방법\n",
    "- activity_regularizer : 출력 값 정규화 방법\n",
    "- kernel_constraint : Optimizer에 의해 업데이트 된 이후에 가중치에 적용되는 부가적인 제약 함수\n",
    "- bias_constraint : Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a8ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.Input(shape=INPUT_SIZE)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters=10,\n",
    "    kernel_size=3,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu\n",
    ")(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.Input(shape=INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate=0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters=10,\n",
    "    kernel_size=3,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu\n",
    ")(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7723e6f",
   "metadata": {},
   "source": [
    "## tf.keras.layers.MaxPool1D\n",
    "\n",
    "합성곱 신경망과 함께 쓰이는 기법 중 하나는 풀링이다.\n",
    "보통 피처 맵의 크기를 줄이거나 주요한 특징을 뽑아내기 위해 합성곱 이후에 적용되는 기법이다.\n",
    "풀링에는 주로 두 가지 풀링 기법이 사용되는데, 맥스 풀링과 평균 풀링이 있다.\n",
    "맥스 풀링은 피처 맵에 대해 최댓값만을 뽑아내는 방식이고,\n",
    "평균 풀링은 피처 맵에 대해 전체 값들을 평균한 값을 뽑는 방식이다.\n",
    "\n",
    "맥스 풀링도 합성곱과 같이 세가지 형태로 모델이 구분돼 있다.\n",
    "MaxPool1D, MaxPool2D, MaxPool3D로 나눠져 있는데 합성곱과 똑같은 원리다.\n",
    "자연어 처리에 주로 사용되는 합성곱과 동일하게 MaxPool1D를 주로 사용하는데 한 방향으로만 풀링이 진행된다.\n",
    "사용법은 앞에서 설명한 합성곱과 동일한다.\n",
    "\n",
    "```python\n",
    "# 방법 1\n",
    "max_pool = tf.keras.layers.MaxPool1D(...)\n",
    "max_pool.apply(input)\n",
    "\n",
    "# 방법 2\n",
    "max_pool = tf.keras.layers.MaxPool1D(...)(input)\n",
    "```\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    pool_size=2, strides=None, padding='valid',\n",
    "    data_format='channels_last', **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- pool_size : 풀링을 적용할 필터의 크기를 뜻한다. 정수값을 받는다.\n",
    "- strides : 적용할 스트라이드의 값. 정수 혹은 None 값을 받는다.\n",
    "- padding : 패딩 방법을 지정한다. \"valid\" 또는 \"same\"을 지정할 수 있다.\n",
    "- data_format : 데이터의 표현 방법을 선택한다. \"channel_list\" 혹은 \"channel_first\"를 지정할 수 있다. channel_list의 경우 데이터는 (batch, length, channels) 형태여야 하고, channel_first의 경우 데이터는 (batch, length, channels) 형태여야 한다.\n",
    "\n",
    "입력값이 합성곱과 맥스 풀링을 사용한 후 완전 연결 계층을 통해 최정 출력 값이 나오는 구조를 만들어 보자.\n",
    "그리고 입력값에는 드롭아웃을 적용한다.\n",
    "그리고 맥스 풀링 결과값을 완전 연결 계층으로 연결하기 위해서는 행렬이었던 것을 벡터로 만들어야 한다.\n",
    "이때 `tf.keras.layers.Flatten`을 사용한다. Flatten의 경우 별다른 인자값 설정 없이도 사용할 수 있기 때문에 쉽게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d91797",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate=0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(dropout)\n",
    "\n",
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=3, padding='same')(conv)\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units=50, activation=tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8a98",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "\n",
    "`tf.keras.Sequential`은 케라스를 활용해 모델을 구축할 수 있는 가장 간단한 형태의 API이다.\n",
    "`Sequential` 모듈을 이용하면 간단한 순차적인 레이어의 스택을 구현할 수 있다.\n",
    "간단한 형태의 완전 연결 계층을 구현해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04bcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bfef6",
   "metadata": {},
   "source": [
    "`Sequential` 인스턴스를 생성한 후 해당 인스턴스에 여러 레이어를 순차적으로 더하기만 하면 모델이 완성된다.\n",
    "이렇게 만든 모델을 입력값을 더한 순서에 맞게 레이어들을 통과시킨 후 최종 출력값을 뽑아오게 된다.\n",
    "`Sequential` 모듈의 경우 위와 같이 구현 자체가 매우 간단하다는 사실을 알 수 있다.\n",
    "그에 반해 모델 구현에 제약이 있는데, 모델의 층들이 순차적으로 구성돼 있지 않은 경우에는 `Sequential` 모듈을 사용해 구현하기가 어려울 수 있다.\n",
    "예를 들면, VQA(Visual Question Answering) 문제(사진과 질문이 입력값으로 주어지고 사진을 참고해 질문에 답하는 문제)의 경우 사진 데이터에서 특징을 뽑는 레이어와 질문 텍스트 데이터에서 특징을 뽑는 두 레이어가 각기 순차적으로 존재한다.\n",
    "따라서 최종적으로 출력값을 뽑기 위해서는 이 두 값을 합쳐야 하는데, 이러한 구조의 모델을 구현할 때 `Sequential`모듈을 사용하게 되면 하나의 플로만 계산할 수 있는 `Sequential` 모듈로는 두 개의 값을 합칠 수가 없기 때문에 여러 제약이 존재한다.\n",
    "따라서 이러한 경우에는 앞으로 소개할 다른 방법을 이용해 모델을 구현하는 것이 적절하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df23467",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "`Sequential` 모듈은 간단한 레이어들의 스택 구조에는 적합하지만 복잡한 모델의 경우에는 여러 구현상의 제약이 있을 수 있다.\n",
    "예를 들면, 모델의 구조가 다음과 같을 경우 `Sequential` 모듈을 사용하기가 어려울 수 있다.\n",
    "- 다중 입력값 모델\n",
    "- 다중 출력값 모델\n",
    "- 공유 층을 활용하는 모델\n",
    "- 데이터 흐름이 순차적이지 않은 모델\n",
    "\n",
    "이러한 모델을 구현할 때는 케라의 `Functional API`를 사용하거나 이후 살펴볼 `Subclassing` 방식을 사용하는 것이 적절할 수 있다.\n",
    "`Functional API`를 활용해 앞에서 정의한 모델과 동일한 모델을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9022e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "predictions = layers.Dense(10,  activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681a418",
   "metadata": {},
   "source": [
    "`Functional API`를 활요하기 위해서는 입력값을 받는 Input 모듈을 선언해야 한다.\n",
    "이 모듈을 선언할 때는 모델의 입력으로 받는 값의 형태를 정의하면 된다.\n",
    "이 Input 모듈을 정의한 후 입력값을 적용할 레이어를 호출할 때 인자로 전달하는 방식으로 구현하면 된다.\n",
    "\n",
    "이처럼 정의한 후 최종 출력값을 사용해 모델을 학습하면 된다.\n",
    "그러면 마지막 출력값이 앞에서 `Sequential`로 구현했을 때의 모델과 동일한 형태가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46993133",
   "metadata": {},
   "source": [
    "## Custom Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771957ad",
   "metadata": {},
   "source": [
    "앞에 두 API를 사용하기 위해 케라스는 `layers` 패키지에 정의된 레이어를 사용해 구현했다.\n",
    "대부분 구현하고자 하는 모델의 경우 해당 패키지에 구현돼 있지만\n",
    "새로운 연산을 하는 레이어 혹은 편의를 위해 여러 레이어를 하나로 묶은 레이어를 구현해야 하는 경우가 있다.\n",
    "이때 사용자 정의 층을 만들어 사용하면 된다.\n",
    "앞에서 정의한 모델에서는 dense 층이 여러 번 사용된 신경망을 사용했다.\n",
    "이 신경망을 하나의 레이어로 묶어 재사용성을 높이고 싶다면 다음과 같이 사용자 정의 층으로 정의하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb278814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class CustomLayer(layers.Layer):\n",
    "    def __init__(self, hidden_dimension, hidden_dimension2, output_dimension):\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.hidden_dimension2 = hidden_dimension2\n",
    "        self.output_dimension = output_dimension\n",
    "        super(CustomLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense_layer1 = layers.Dense(self.hidden_dimension, activation='relu')\n",
    "        self.dense_layer2 = layers.Dense(self.hidden_dimension2, activation='relu')\n",
    "        self.dense_layer3 = layers.Dense(self.output_dimension, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_layer1(inputs)\n",
    "        x = self.dense_layer2(x)\n",
    "        return self.dense_layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fa931",
   "metadata": {},
   "source": [
    "사용자 정의 층을 정의할 때는 `layers` 패키지의 Layer 클래스를 상속받고 위와 같이 3개의 메소드를 정의하면 된다.\n",
    "우선 하이퍼파라미터는 객체를 생성할 때 호출되도록 `__init__` 메소드에서 정의하고,\n",
    "모델의 가중치와 관련된 값은 `build` 메소드에서 생성되도록 정의한다.\n",
    "그리고 이렇게 정의한 값들을 이용해 `call` 메소드에서 해당 층의 로직을 정의하면 된다.\n",
    "이렇게 정의한 사용자 정의 층은 `Sequential API` 나 `Functional API`를 활용할 때 하나의 층으로 사용할 수 있다.\n",
    "만약 `Sequentail` 모듈을 활용한다면 다음과 같이 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71828668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(CustomLayer(64, 64, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33737da3",
   "metadata": {},
   "source": [
    "## Subclassing (Custom Model)\n",
    "\n",
    "이번에는 가장 자유도가 높은 `Subclassing`을 알아보자.\n",
    "이 경우 `tf.keras.Model`을 상속받고 모델 내부 연산들을 직접 구현하면 된다.\n",
    "모델 클래스를 구현할 때는 객체를 생성할 때 호출되는 `__init__` 메소드와\n",
    "생성된 인스턴스를 호출할 때를 구현할 때(즉, 모델 연산이 사용될 때) 호출되는 `call` 메소드만 구현하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b61609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, hidden_dimenstion, hidden_dimenstion2, output_dimension):\n",
    "        super(MyModel, self).__init__(name='my model')\n",
    "        self.dense_layer1 = layers.Dense(hidden_dimenstion, activation='relu')\n",
    "        self.dense_layer1 = layers.Dense(hidden_dimenstion2, activation='relu')\n",
    "        self.dense_layer3 = layers.Dense(output_dimension, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_layer1(inputs)\n",
    "        x = self.dense_layer2(x)\n",
    "        return self.dense_layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e966a",
   "metadata": {},
   "source": [
    "구현 방법은 사용자 정의 층을 만드는 방식과 매우 유사하다.\n",
    "그뿐만 아니라 이 방법은 파이토치 프레임워크에서 모델을 구현할 때 사용하는 방식과도 매우 유사하다.\n",
    "`__init__` 메소드에서는 모델에 사용될 층과 변수를 정의하면 되고,\n",
    "`call` 메소드에서는 이렇게 정의한 내용을 활용해 모델 연산을 진행하면 된다.\n",
    "참고로 모델에서 사용될 층을 정의할 때도 사용자 정의 층을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bccbc",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "\n",
    "이제 모델 학습에 대해서 알아보자.\n",
    "모델 학습이라고 하지만, 실제로는 학습뿐 아니라 모델 검증, 예측 등 여러 과정을 포함한다는 점을 알아두자.\n",
    "텐서플로 2.0 공식 가이드에서 모델 학습에 대해 권장하는 방법은 크게 두가지로 나뉜다.\n",
    "1. 케라스 모델의 내장 API를 활용하는 방법\n",
    "2. 학습, 검증, 예측 등 모든 과정을 `GradientTape` 객체를 활용해 직접 구현하는 방법\n",
    "\n",
    "첫번째 방법의 경우 대부분 케라스 모델의 메소드로 이미 구현돼 있어 간편하다는 큰 장점이 있고,\n",
    "두번째 방법의 경우 첫번째 방법과 비교했을 때 일일이 구현해야 한다는 단점이 있지만 좀 더 복잡한 로직을 유연하고\n",
    "자유롭게 구현할 수 있다는 장점이 있다.\n",
    "\n",
    "우리는 주로 첫번째 방법으로 공부할 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fa311",
   "metadata": {},
   "source": [
    "### 내장 API를 활용하는 방법\n",
    "\n",
    "이미 정의된 케라스의 모델 객체가 있다고 가정해보자.\n",
    "이 모델 객체는 케라스의 모델 객체이기 때문에 여러 메소드가 이미 내장돼 있다.\n",
    "따라서 내장 메소드를 간단히 사용하기만 하면 된다.\n",
    "먼저 해야 할 일은 학습 과정을 정의하는 것이다.\n",
    "즉, 학습 과정에서 사용될 손실 함수, 오티마이저, 평가에 사용될 지표 등을 정의하면 된다.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.mertics.Accuracy()]\n",
    ")\n",
    "```\n",
    "\n",
    "위와 같이 정의하면 학습을 실행할 모든 준비가 끝난다.\n",
    "참고로 옵티마이저, 손실 함수, 평가 지표 등은 객체 형식으로 지정해도 되고\n",
    "다음과 같이 문자열 형태로 지정해도 된다.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "이제 정의된 모델 객체를 대상으로 학습, 평가, 예측 메소드를 호출하면 정의한 값들을 활용해 학습이 진행된다.\n",
    "즉, 다음과 같이 `fit` 메소드를 호출하면 데이터들이 모델을 통과하며 학습이 진행된다.\n",
    "아울러 학습이 진행되면서 각 에폭당 모델의 성능(손실함수, 정확도) 등이 출력되는 것을 확인할 수 있다.\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3\n",
    ")\n",
    "```\n",
    "\n",
    "학습 과정에 있어서 각 에폭마다 검증을 진행하는 것 또한 가능하다.\n",
    "`evaluate` 메소드를 사용해 검증할 수 있지만 매번 에폭을 호출해야 한다는 번거로움이 있다.\n",
    "따라서 에폭마다 검증 결과를 보기 위해서는 `fit` 함수에 검증 데이터를 추가로 넣으면 된다.\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "```\n",
    "\n",
    "이제 모델을 구축하는 과정과 학습하는 과정을 모두 알아봤다.\n",
    "이어서 간단한 더미 데이터를 활용해 감정 분석 문제를 해결해 보자,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c63e3",
   "metadata": {},
   "source": [
    "### 더미 데이터를 활용한 감정 분석 모델링\n",
    "\n",
    "[출처](https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/blob/master/2.NLP_PREP/2.1.2.tensorflow2.ipynb)\n",
    "\n",
    "이번에 구현할 모델은 심층 신경망 구조를 사용해 앞서 텍스트의 긍정/부정을 예측하는 감정 분석 모델이다.\n",
    "감정 분석 문제에 대한 자세한 내용은 이후에 다시 소개할 것이며, 모델에 적용할 데이터는 앞에서 정의한 테스트 데이터를 사용한다.\n",
    "모델의 자세한 구조는 다음과 같다.\n",
    "\n",
    "<img src=\"심층신경망.png\" alt=\"심층신경망\" style=\"width: 300px;\"/>\n",
    "\n",
    "우선 각 단어로 구성된 입력값은 임베딩된 벡터로 변형된다.\n",
    "이후 각 벡터를 평균해서 하나의 벡터로 만든다.\n",
    "이후에 하나의 은닉층을 거친 후 하나의 결괏값을 뽑는 구조다.\n",
    "마지막으로 나온 결괏값에 시그모이드 함수를 적용해 0과 1사이의 값을 구한다.\n",
    "\n",
    "모델에서 나온 임베딩 벡터 등과 같은 내용이 잘 이해되지 않더라도 대략적인 모델의 구조가 위와 같이 구성된다는 것만 이해하면 된다.\n",
    "이후 자세한 내용은 다음 장에서 알려주도록 하겠다.\n",
    "이번 장에서는 텐서플로 2.0버전에서 케라스를 이용해 모델을 구현하는 방법을 중심적으로 살펴보자.\n",
    "\n",
    "이제 본격적으로 모델을 구현해 보자.\n",
    "데이터는 이전 장에서 사용했던 텍스트 데이터를 그대로 사용하고, 동일하게 전처리 과정을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc32e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = [\n",
    "    '너 오늘 이뻐 보인다',\n",
    "    '나는 오늘 기분이 더러워',\n",
    "    '끝내주는데, 좋은 일이 있나봐',\n",
    "    '나 좋은 일이 생겼어',\n",
    "    '아 오늘 진짜 짜증나',\n",
    "    '환성적인데, 정말 좋은거 같아'\n",
    "]\n",
    "\n",
    "targets= [[1], [0], [1], [1], [0], [1]]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "\n",
    "input_sequences = np.array(sequences)\n",
    "labels = np.array(targets)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82281d55",
   "metadata": {},
   "source": [
    "전처리 과정은 앞에서 다뤘기 때문에 설명은 생략한다.\n",
    "추가로 모델 구축 및 모델 학습에 필요한 변수를 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2ddc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 100\n",
    "vocab_size = len(word_index) + 1\n",
    "emb_size = 128\n",
    "hidden_dimension = 256\n",
    "output_dimension = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c509774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, emb_size, input_length=4))\n",
    "model.add(layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(layers.Dense(hidden_dimension, activation='relu'))\n",
    "model.add(layers.Dense(output_dimension, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57be91",
   "metadata": {},
   "source": [
    "`Sequential` 객체를 생성한 후 각 층을 추가하면 된다.\n",
    "혹은 객체 생성 시 안자로 사용할 층을 순차적으로 리스트로 만들어서 전달하는 방법으로도 위와 동일한 모델을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3477ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, emb_size, input_length=4),\n",
    "    layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    layers.Dense(hidden_dimension, activation='relu'),\n",
    "    layers.Dense(output_dimension, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a0522",
   "metadata": {},
   "source": [
    "구현한 모델을 보면 입력값을 임베딩하는 Embedding 층을 모델에 추가했다.\n",
    "이후 임베딩 된 각 단어의 벡터를 평균하기 위해 람다 층을 사용한다.\n",
    "람다 층은 텐서플로 연산을 `Sequential API`와 `Funcational API`에 적용하기 위해 사용하는 방법이다.\n",
    "평균의 경우 하나의 층으로 정의돼 있지 않기 때문에 람다 층을 활용해 해당 층에 들어오는 입력값들을 평균한다.\n",
    "람다 층을 활용해 평균을 낸 후 하나의 은닉층을 통과한 후 최종 출력값을 뽑기 위해 두개의 Dense 층을 모델에 추가한다.\n",
    "이때 최종 출력값을 뽑은 Dense 층의 경우 0과 1사이의 확률값을 뽑기 위해 활성화 함수를 시그모이드 함수로 정의한다.\n",
    "\n",
    "이렇게 최종 출력 층까지 모델에 추가헀다면 모델이 모두 완성된 것이다.\n",
    "모델을 구축하는 과정이 모두 끝났으니 모델을 학습해보자.\n",
    "케라스 내장 API를 활용해 모델을 학습하기 위해서는 우선 모델을 컴파일하는 메소드를 통해 학습 과정을 정의한다.\n",
    "옵티마이저의 경우 아담 최적화 알고리즘을 사용하고, 학습은 이진 분류 문제이므로 이진 교차 엔트로피 손실함수를 사용한다.\n",
    "그리고 추가로 모델의 성능을 측정하기 위한 기준인 평가 지표를 정의하는데, 이진 분류의 평가 지표로 가장 널리 사용되는 정확도를 평가 지표로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9278313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933867fe",
   "metadata": {},
   "source": [
    "이처럼 컴파일 메소드로 학습 과정을 정의했다면 케라스의 내장 API를 통해 학습할 준비가 모두 끝났다.\n",
    "이제 `fit` 메소드를 통해 학습을 진행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66e360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 128)            2688      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 35,969\n",
      "Trainable params: 35,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9310556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 802us/step - loss: 0.6914 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 601us/step - loss: 0.6757 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 718us/step - loss: 0.6576 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 673us/step - loss: 0.6339 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 924us/step - loss: 0.6118 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 795us/step - loss: 0.5908 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 674us/step - loss: 0.5747 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 697us/step - loss: 0.5492 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 762us/step - loss: 0.4675 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 696us/step - loss: 0.4209 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 589us/step - loss: 0.3742 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 818us/step - loss: 0.3389 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 887us/step - loss: 0.2684 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 958us/step - loss: 0.2093 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 746us/step - loss: 0.1450 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 754us/step - loss: 0.1186 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 679us/step - loss: 0.0765 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 817us/step - loss: 0.0548 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 734us/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 713us/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 808us/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 888us/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 674us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 763us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 647us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 817us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 700us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 641us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 594us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 630us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 679us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 667us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 641us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 641us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 584us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 653us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 687us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 647us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 574us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 646us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 679us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 667us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 593us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 677us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 602us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 592us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 738us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 556us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 616us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 580us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 575us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 599us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 748us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 742us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 597us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 574us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 589us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 801us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 604us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 590us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 580us/step - loss: 9.8889e-04 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 565us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 593us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 634us/step - loss: 9.7576e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 754us/step - loss: 9.4225e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 739us/step - loss: 9.6795e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 671us/step - loss: 9.2220e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.1970e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 8.2887e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 899us/step - loss: 9.7214e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 9.5640e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 620us/step - loss: 7.4457e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 649us/step - loss: 8.1502e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 691us/step - loss: 8.7157e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.1326e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 840us/step - loss: 5.7772e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 900us/step - loss: 7.3262e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 999us/step - loss: 7.0776e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.8978e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 979us/step - loss: 5.8660e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 7.5488e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.9216e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.8706e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 6.5241e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 4.9503e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 566us/step - loss: 4.6561e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 538us/step - loss: 4.8283e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 549us/step - loss: 4.9329e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 561us/step - loss: 5.1023e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163c53a30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5326fe0",
   "metadata": {},
   "source": [
    "`fit` 메소드를 호출하면 학습 경과가 출력되는 것을 확인할 수 있다.\n",
    "에폭이 경과함에 따라 손실함수의 값과 정확도가 늘어나는 것을 확인할 수 있다.\n",
    "더미 데이터의 결과이기 때문에 의미 있는 수치는 아니지만 텐서플로 2.0의 표준 방법을 통해 학습이 정상적으로 돌아가도록 구현했다는 데 의미가 있다.\n",
    "\n",
    "이제 `Sequential API`가 아닌 `Functional API`, `Subclassing` 방법으로 동일한 모델을 구현해보고 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a94a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (4, ))\n",
    "embed_output = layers.Embedding(vocab_size, emb_size)(inputs)\n",
    "pooled_output = tf.reduce_mean(embed_output, axis=1)\n",
    "hidden_layer = layers.Dense(hidden_dimension, activation='relu')(pooled_output)\n",
    "outputs = layers.Dense(output_dimension, activation='sigmoid')(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd773fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9813e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 4, 128)            2688      \n",
      "_________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLam (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 35,969\n",
      "Trainable params: 35,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7817aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 899us/step - loss: 0.6964 - accuracy: 0.2292\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 811us/step - loss: 0.6792 - accuracy: 0.7292\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 728us/step - loss: 0.6668 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 895us/step - loss: 0.6485 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6314 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 813us/step - loss: 0.6143 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 756us/step - loss: 0.5899 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 689us/step - loss: 0.5708 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 699us/step - loss: 0.4983 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 681us/step - loss: 0.4431 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 660us/step - loss: 0.4116 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 636us/step - loss: 0.3663 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 658us/step - loss: 0.3257 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 631us/step - loss: 0.1896 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 605us/step - loss: 0.1478 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 605us/step - loss: 0.1441 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 742us/step - loss: 0.0932 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 748us/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 645us/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 865us/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 782us/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 898us/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 819us/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 668us/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 712us/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 697us/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 667us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 926us/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 786us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 622us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 594us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 617us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 638us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 989us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 681us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 643us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 727us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 567us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 644us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 686us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 605us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 629us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 631us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 879us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 878us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 744us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 667us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 648us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 555us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 849us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 821us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 598us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 601us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 571us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 697us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 628us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 765us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 685us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 598us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 693us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 832us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 948us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 602us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 599us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 626us/step - loss: 9.7539e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 859us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 700us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 617us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 641us/step - loss: 8.7970e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 614us/step - loss: 9.4926e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 528us/step - loss: 8.4148e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 560us/step - loss: 9.4495e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 919us/step - loss: 9.6952e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 872us/step - loss: 9.4941e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 713us/step - loss: 8.7437e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 587us/step - loss: 8.8295e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 679us/step - loss: 6.9041e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 893us/step - loss: 8.8982e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 877us/step - loss: 7.9568e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 778us/step - loss: 8.9803e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 700us/step - loss: 8.1174e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 715us/step - loss: 6.4702e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.0170e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 738us/step - loss: 7.9091e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 882us/step - loss: 6.7832e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 727us/step - loss: 7.5258e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 741us/step - loss: 6.7824e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 646us/step - loss: 5.4027e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 566us/step - loss: 6.8071e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 675us/step - loss: 6.3081e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 668us/step - loss: 5.5922e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 573us/step - loss: 5.2645e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166b65160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fe8df",
   "metadata": {},
   "source": [
    "앞에서 알아본 것처럼 `Funcational API`는 케라스의 입력층을 구현한 후 각 값을 다음 레이어들을 호출하면서 인자로 넣는 방식으로 구현하면 된다.\n",
    "구현 방법에서 차이가 있을 뿐 모델 연산 흐름이나 로직이 변경된 것은 아니므로 결과는 동일하게 나온다.\n",
    "다음은 `Subclassing` 방법으로 동일한 모델을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da62314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dimension, hidden_dimension, output_dimension):\n",
    "        super(CustomModel, self).__init__(name='my_model')\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dimension)\n",
    "        self.dense_layer = layers.Dense(hidden_dimension, activation='relu')\n",
    "        self.output_layer = layers.Dense(output_dimension, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4540827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 806us/step - loss: 0.6995 - accuracy: 0.2917\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 754us/step - loss: 0.6800 - accuracy: 0.8542\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 661us/step - loss: 0.6665 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 683us/step - loss: 0.6445 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 682us/step - loss: 0.6384 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 691us/step - loss: 0.6161 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 670us/step - loss: 0.5983 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 654us/step - loss: 0.5059 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 646us/step - loss: 0.4598 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 685us/step - loss: 0.4209 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 565us/step - loss: 0.3936 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 839us/step - loss: 0.3151 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 983us/step - loss: 0.2945 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 638us/step - loss: 0.2256 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 607us/step - loss: 0.2090 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 615us/step - loss: 0.1564 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 694us/step - loss: 0.1161 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 968us/step - loss: 0.0909 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 787us/step - loss: 0.0904 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 747us/step - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 636us/step - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 683us/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 606us/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 697us/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 806us/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 893us/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 661us/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 684us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 655us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 652us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 650us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 654us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 585us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 585us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 638us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 675us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 637us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 952us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 589us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 610us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 587us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 569us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 761us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 616us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 595us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 667us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 580us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 653us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 795us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 799us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 547us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 621us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 564us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 517us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 837us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 583us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 852us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 604us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 595us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 561us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 571us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 565us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 598us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 537us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 569us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 782us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 584us/step - loss: 9.1849e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 566us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 549us/step - loss: 8.4255e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 534us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 583us/step - loss: 9.4379e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 723us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.1963e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 761us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 756us/step - loss: 8.4023e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 681us/step - loss: 8.5511e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 617us/step - loss: 7.8892e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 947us/step - loss: 8.1603e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 585us/step - loss: 7.9553e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 738us/step - loss: 6.5540e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 749us/step - loss: 7.7935e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 827us/step - loss: 8.4286e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 861us/step - loss: 6.9317e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 619us/step - loss: 7.1112e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 860us/step - loss: 7.5013e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 759us/step - loss: 7.7558e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 884us/step - loss: 6.0006e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 992us/step - loss: 5.4703e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 952us/step - loss: 6.5511e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.0596e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5.1845e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 591us/step - loss: 5.4037e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 562us/step - loss: 5.7616e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x166ec4b20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel(vocab_size = vocab_size,\n",
    "            embed_dimension=emb_size,\n",
    "            hidden_dimension=hidden_dimension,\n",
    "            output_dimension=output_dimension)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c1031",
   "metadata": {},
   "source": [
    "### keras custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aead704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, hidden_dimension, output_dimension, **kwargs):\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense_layer1 = layers.Dense(self.hidden_dimension, activation = 'relu')\n",
    "        self.dense_layer2 = layers.Dense(self.output_dimension)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_output = self.dense_layer1(inputs)\n",
    "        return self.dense_layer2(hidden_output)\n",
    "\n",
    "    # Optional\n",
    "    def get_config(self):\n",
    "        base_config = super(CustomLayer, self).get_config()\n",
    "        base_config['hidden_dim'] = self.hidden_dimension\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e9ab241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 736us/step - loss: 0.6936 - accuracy: 0.5833\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 658us/step - loss: 0.6761 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 598us/step - loss: 0.6575 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 628us/step - loss: 0.6453 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 558us/step - loss: 0.6252 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 564us/step - loss: 0.5986 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 839us/step - loss: 0.5724 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 795us/step - loss: 0.5190 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 732us/step - loss: 0.4740 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 679us/step - loss: 0.4335 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 685us/step - loss: 0.3937 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 695us/step - loss: 0.2822 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 649us/step - loss: 0.2322 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 726us/step - loss: 0.1816 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 680us/step - loss: 0.1567 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 652us/step - loss: 0.1149 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 818us/step - loss: 0.0948 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 689us/step - loss: 0.0635 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 688us/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 620us/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 625us/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 814us/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 934us/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 815us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 747us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 758us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 715us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 599us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 773us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 566us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 921us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 781us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 808us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 885us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 622us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 637us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 625us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 711us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 776us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 612us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 613us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 990us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 583us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 624us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 588us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 639us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 948us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 654us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 587us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 595us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 662us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 594us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 709us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 723us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 914us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 613us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 627us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 576us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 606us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 592us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 569us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 548us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 587us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 572us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 623us/step - loss: 9.4646e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 740us/step - loss: 9.7395e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 613us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 610us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 537us/step - loss: 9.1537e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 619us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 588us/step - loss: 9.1082e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 623us/step - loss: 8.9576e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 866us/step - loss: 8.7122e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 598us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 676us/step - loss: 8.8844e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 644us/step - loss: 7.5674e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 586us/step - loss: 7.9176e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 616us/step - loss: 6.9117e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 548us/step - loss: 8.0018e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7.3212e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 611us/step - loss: 7.6730e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 589us/step - loss: 7.8231e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 739us/step - loss: 7.3457e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 714us/step - loss: 6.5532e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6.9404e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 762us/step - loss: 6.3397e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 637us/step - loss: 6.6866e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 829us/step - loss: 6.1895e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 790us/step - loss: 6.0784e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 877us/step - loss: 6.2942e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 727us/step - loss: 5.7526e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 630us/step - loss: 6.3308e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167b43580>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, emb_size, input_length = 4),\n",
    "    layers.Lambda(lambda x: tf.reduce_mean(x, axis = 1)),\n",
    "    CustomLayer(hidden_dimension, output_dimension),\n",
    "    layers.Activation('sigmoid')])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79a7eb",
   "metadata": {},
   "source": [
    "# 사이킷런\n",
    "\n",
    "사이킷런은 파이썬용 머신러닝 라이브러리이다.\n",
    "머신러닝 기술을 활용하는 데 필요한 다양한 기능을 제공하며, 파이썬으로 머신러닝 모델을 만들 수 있는 최적의 라이브러리다.\n",
    "라이브러리를 구성하는 대부분의 모듈들이 통일된 인터페이스를 가지고 있어 간단하게 여러 기법을 적용할 수 있으며, 쉽고 빠르게 원하는 결과를 얻을 수 있다.\n",
    "\n",
    "딥러닝 모델을 텐서플로, 케라스, 파이토치 등을 이용해서 생성할 수 있는 것처럼 머신러닝 모델은 주로 사이킷런 라이브러리를 통해 만들어 낼 수 있다.\n",
    "사이킷런 라이브러리는 지도 학습을 위한 모듈, 비지도 학습을 위한 모듈, 모델 선택 및 평가를 위한 모듈,\n",
    "데이터 변환 및 데이터를 불러오기 위한 모듈, 계산 성능 향상을 위한 모듈로 구성돼 있다.\n",
    "\n",
    "지도 학습 모듈에는 나이브 베이즈, 의사결정 트리, 서포트 벡터 머신 모델 등이 있다.\n",
    "비지도 학습 모듈에는 군집화, 가우시안 혼합 모델 등이 있다.\n",
    "\n",
    "모델 선택과 평가 모듈에서는 교차 검증, 모델 평가, 모델의 지속성을 위해 모델 저장과 불러오기를 위한 기능 등을 제공한다.\n",
    "그리고 데이터 변환 모듈에서는 파이프라인, 특징 추출, 데이터 전처리, 차원 축소 등의 기능을 제공한다.\n",
    "\n",
    "또한 머신러닝 연구와 학습을 위해 라이브러리 안에 자체적으로 데이터 셋을 포함하고 있고, 이를 쉽게 불러와서 사용할 수 있다.\n",
    "라이브러리에서 기본적으로 제공되는 데이터로는 당뇨병 데이터, 아이리스 데이터, 유방암 데이터 등이 있다.\n",
    "\n",
    "머신러닝을 통해 문제를 해결하기 위해서는 해결해야 할 문제에 적합한 알고리즘을 선택하는 것이 매우 중요한데\n",
    "사이킷런에서 제공하는 아래의 그림을 참고하면 좀 더 쉽게 문제를 해결하는 데 적합한 모델을 선택할 수 있다.\n",
    "\n",
    "<img src=\"scikit_learn.png\" alt=\"scikit_learn\" style=\"width: 500px;\"/>\n",
    "\n",
    "이번 장에서는 전체적인 사이킷런의 사용법을 알아본다.\n",
    "먼저 지도 학습 모듈에 대해 알아본 후 비지도 학습 모듈에 대해 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa1c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
